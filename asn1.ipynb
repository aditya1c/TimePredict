{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import median\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt', encoding='UTF-8'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u, g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation sets\n",
    "raw_data = []\n",
    "for user, game, d in readJSON(\"train.json.gz\"):\n",
    "    raw_data.append([user, game, d])\n",
    "\n",
    "random.shuffle(raw_data)    \n",
    "    \n",
    "raw_train = raw_data[:175000]\n",
    "raw_valid = raw_data[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict with user as key and all games played by him as values\n",
    "gamesOfUser = defaultdict(set)\n",
    "for item in raw_data:\n",
    "    gamesOfUser[item[0]].add(item[1])\n",
    "\n",
    "#Creating a set with all the games\n",
    "allGames = set()\n",
    "# allGamesList = sorted(list(allGames))\n",
    "# allGamesDict = {}\n",
    "# for i, g in enumerate(allGamesList):\n",
    "#     allGamesDict[g] = i/len(allGamesList)\n",
    "    \n",
    "for item in raw_data:\n",
    "    allGames.add(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175000, 175000)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPos = []\n",
    "for item in raw_train:\n",
    "    tup = (item[0], item[1], 1)\n",
    "    trainPos.append(tup)\n",
    "    \n",
    "trainNeg = []\n",
    "for z in range(1):\n",
    "    for item in raw_train:\n",
    "        user = item[0]\n",
    "        games = gamesOfUser[user]\n",
    "        notPlayedGames = allGames - games\n",
    "        randomOne = random.sample(notPlayedGames, 1)[0]\n",
    "        tup = (user, randomOne, 0)\n",
    "        trainNeg.append(tup)\n",
    "\n",
    "len(trainPos), len(trainNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying validation set\n",
    "valid = []\n",
    "for item in raw_valid:\n",
    "    tup = (item[0], item[1], 1)\n",
    "    valid.append(tup)\n",
    "\n",
    "#Creating a negative validation elements and appending to validation set (valid)\n",
    "for item in raw_valid:\n",
    "    user = item[0]\n",
    "    games = gamesOfUser[user]\n",
    "    notPlayedGames = allGames - games\n",
    "    randomOne = random.sample(notPlayedGames, 1)[0]\n",
    "    tup = (user, randomOne, 0)\n",
    "    valid.append(tup)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Would-play baseline: just rank which games are popular and which are not, and return '1' if a game is among the top-ranked\n",
    "\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user, game, _ in raw_train:\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort(reverse = True)\n",
    "\n",
    "maxi = 0\n",
    "for item in mostPopular:\n",
    "    maxi = max(maxi, item[0])\n",
    "\n",
    "mostPopularDict = defaultdict(float)\n",
    "for item in mostPopular:\n",
    "    mostPopularDict[item[1]] = item[0]/maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 2 dicts. One with game and one with user\n",
    "\n",
    "gamesOfUser = defaultdict(set)\n",
    "usersOfGame = defaultdict(set)\n",
    "\n",
    "#Only on training set\n",
    "for item in raw_train:\n",
    "    gamesOfUser[item[0]].add(item[1])\n",
    "    usersOfGame[item[1]].add(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(u, g):\n",
    "    similarities1 = []\n",
    "    users = usersOfGame[g]\n",
    "    for g2 in gamesOfUser[u]:\n",
    "        if g == g2: continue # other than the query\n",
    "        sim = Jaccard(users, usersOfGame[g2])\n",
    "        similarities1.append(sim)\n",
    "    \n",
    "#     similarities2 = []\n",
    "#     games = gamesOfUser[u]\n",
    "#     for u2 in usersOfGame[g]:\n",
    "#         if u == u2: continue # other than the query\n",
    "#         sim = Jaccard(games, gamesOfUser[u2])\n",
    "#         similarities2.append(sim)\n",
    "    \n",
    "#     count = 0\n",
    "#     #Thershold set as 0.008\n",
    "#     threshold = 0.008\n",
    "#     for sim in similarities:\n",
    "#         if sim > threshold:\n",
    "#             count += 1\n",
    "    \n",
    "    #Since it is given most of the values should cross. Assuming it to be more than 50%.\n",
    "    #Can be done using median too\n",
    "#     if count > 0.5*len(similarities):\n",
    "#         return 1\n",
    "\n",
    "#     if len(similarities1) > 0:\n",
    "#         min1 = min(similarities1)\n",
    "#     else:\n",
    "#         min1 = 0.002459177358905974\n",
    "    \n",
    "    \n",
    "    \n",
    "#     max1 = max(similarities1) if len(similarities1) > 0 else 0.036477232467427144\n",
    "    mean1 = sum(similarities1)/len(similarities1) if len(similarities1) > 0 else 0.013651729456640182\n",
    "    #median1 = median(similarities1) if len(similarities1) > 0 else 0\n",
    "    \n",
    "#     min2 = min(similarities2) if len(similarities2) > 0 else 0.008970185399297994\n",
    "#     max2 = max(similarities2) if len(similarities2) > 0 else 0.0960382978251585\n",
    "#     mean2 = sum(similarities2)/len(similarities2) if len(similarities2) > 0 else 0.03420200386502156\n",
    "    #median2 = median(similarities2) if len(similarities2) > 0 else 0\n",
    "    \n",
    "#     pop = mostPopularDict[g]\n",
    "#     if pop == 0:\n",
    "#         pop = 0.12058540079119132\n",
    "    \n",
    "#     senti = [0, 0, 0, 0]\n",
    "#     if g in sentiDict:\n",
    "#         senti = sentiDict[g]\n",
    "    \n",
    "    return mean1 #, min2, max2, mean2+ senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "for tup in trainPos:\n",
    "    trainX.append(mostSimilar(tup[0], tup[1]))\n",
    "    trainY.append(tup[2])\n",
    "    \n",
    "for tup in trainNeg:\n",
    "    trainX.append(mostSimilar(tup[0], tup[1]))\n",
    "    trainY.append(tup[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "validX = []\n",
    "for item in valid:\n",
    "    validX.append(mostSimilar(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 86.19\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in validX:\n",
    "    if i > 0.009852:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "        \n",
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Played.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Played_Out.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    tmp = mostSimilar(u, g)\n",
    "    res = 0\n",
    "    if tmp > 0.00986:\n",
    "        res = 1\n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (Both Jaccard and Popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 0\n",
    "\n",
    "predictions = []\n",
    "for item in valid:\n",
    "    #Only checking Jaccard if it is popular in the first place\n",
    "    if item[1] in return1:\n",
    "        predictions.append(mostSimilar(item[0], item[1]))\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "    \n",
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (Test Set and Submitting to Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Played.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Played_Final.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    if g in return1:\n",
    "        res = mostSimilar(u, g)\n",
    "    else:\n",
    "        res = 0\n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitted to Kaggle')\n",
    "print('\\nKaggle Username: aditya1c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON1(path):\n",
    "    for l in gzip.open(path, 'rt', encoding='UTF-8'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u, g, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation sets\n",
    "dataset = []\n",
    "for user, game, d in readJSON1(\"train.json.gz\"):\n",
    "    dataset.append([user, game, d['hours_transformed']])\n",
    "    \n",
    "train = dataset[:165000]\n",
    "valid = dataset[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursPerUser = defaultdict(list) \n",
    "hoursPerGame = defaultdict(list)\n",
    "allHours = []\n",
    "\n",
    "for user, game, h in train:\n",
    "    allHours.append(h)\n",
    "    #These variables are not used much. Just to maintain consistency with workbook code\n",
    "    hoursPerUser[user].append(h)\n",
    "    hoursPerGame[game].append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)\n",
    "nUsers = len(hoursPerUser)\n",
    "nGames = len(hoursPerGame)\n",
    "users = list(hoursPerUser.keys())\n",
    "games = list(hoursPerGame.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sum(allHours)/len(allHours)\n",
    "userBiases = defaultdict(float)\n",
    "gameBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Functions which are similar to those discussed in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, game):\n",
    "    return alpha + userBiases[user] + gameBiases[game]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global gameBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    gameBiases = dict(zip(games, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(u, g) for u, g, h in dataset]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in gameBiases:\n",
    "        cost += lamb*gameBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dGameBiases = defaultdict(float)\n",
    "    for u, g, h in dataset:\n",
    "        u,i = u, g\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - h\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dGameBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in gameBiases:\n",
    "        dGameBiases[i] += 2*lamb*gameBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dGameBiases[i] for i in games]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on training set with lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nGames),\n",
    "                             derivative, args = (allHours, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHoursValid = []\n",
    "for u, g, h in valid:\n",
    "    allHoursValid.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to use if user or game are not found in test set\n",
    "ratingMean = sum(allHoursValid)/len(allHoursValid)\n",
    "ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsValid = []\n",
    "for u, g, h in valid:\n",
    "    predictionsValid.append(prediction(u, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseValid = MSE(predictionsValid, allHoursValid)\n",
    "print('MSE on Validation Set with lambda as 1 is ' + str(mseValid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting using lambda and key\n",
    "sorted_users = sorted(userBiases.items(), key = lambda x: x[1])\n",
    "smallestUser = sorted_users[0]\n",
    "largestUser = sorted_users[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_games = sorted(gameBiases.items(), key = lambda x: x[1])\n",
    "smallestGame = sorted_games[0]\n",
    "largestGame = sorted_games[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Smallest User Bias: ' + str(smallestUser))\n",
    "print('\\nLargest User Bias: ' + str(largestUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Smallest Game Bias: ' + str(smallestGame))\n",
    "print('\\nLargest Game Bias: ' + str(largestGame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for lambda from 0 to 1 in steps of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLambda = 0\n",
    "bestMSE = float('inf')\n",
    "lambdaList = []\n",
    "mseList = []\n",
    "\n",
    "lamb = 0\n",
    "step = 0.05\n",
    "#Training and checking using Lambda from 0 to 1\n",
    "for i in range(int(1/step) + 1):\n",
    "    tmpLamb = lamb + 0.05*i\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nGames),\n",
    "                             derivative, args = (allHours, tmpLamb), disp = False)\n",
    "    predictionsValid = []\n",
    "    for u, g, h in valid:\n",
    "        predictionsValid.append(prediction(u, g))\n",
    "    mseValid = MSE(predictionsValid, allHoursValid)\n",
    "    lambdaList.append(tmpLamb)\n",
    "    mseList.append(mseValid)\n",
    "    if mseValid < bestMSE:\n",
    "        bestMSE = mseValid\n",
    "        bestLambda = tmpLamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block output is run locally but not showed it here as the output is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Lambda: ' + str(bestLambda))\n",
    "print('Best MSE: ' + str(bestMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambdaList, mseList)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Variation of MSE with Lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model is used to predict the test set and uploaded to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Hours.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Hours_Final.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    res = prediction(u, g)\n",
    "    #This can be replaced with 0 if required too. Just used for checking in Kaggle\n",
    "    if u not in userBiases or g not in gameBiases:\n",
    "        res = ratingMean\n",
    "        \n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitted to Kaggle')\n",
    "print('\\nKaggle Username: aditya1c')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
