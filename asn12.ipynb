{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import array\n",
    "import gzip\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt', encoding='UTF-8'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u, g, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation sets\n",
    "raw_data = []\n",
    "for user, game, d in readJSON(\"train.json.gz\"):\n",
    "    raw_data.append([user, game, d])\n",
    "\n",
    "random.shuffle(raw_data)    \n",
    "    \n",
    "raw_train = raw_data[:165000]\n",
    "raw_valid = raw_data[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiDict = {}\n",
    "# for line in raw_data:\n",
    "#     t = line[2]['text']\n",
    "#     senti1 = vader.polarity_scores(t)\n",
    "#     senti = []\n",
    "#     senti.append(senti1['neg'])\n",
    "#     senti.append(senti1['neu'])\n",
    "#     senti.append(senti1['pos'])\n",
    "#     senti.append(senti1['compound'])\n",
    "    \n",
    "#     tmp = line[2]['early_access']\n",
    "#     e = 0\n",
    "#     if tmp == True:\n",
    "#         e = 1\n",
    "    \n",
    "#     senti.append(e)\n",
    "    \n",
    "#     sentiDict[line[1]] = senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dict with user as key and all games played by him as values\n",
    "gamesOfUser = defaultdict(set)\n",
    "for item in raw_data:\n",
    "    gamesOfUser[item[0]].add(item[1])\n",
    "\n",
    "#Creating a set with all the games\n",
    "allGames = set()\n",
    "# allGamesList = sorted(list(allGames))\n",
    "# allGamesDict = {}\n",
    "# for i, g in enumerate(allGamesList):\n",
    "#     allGamesDict[g] = i/len(allGamesList)\n",
    "    \n",
    "for item in raw_data:\n",
    "    allGames.add(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165000, 165000)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPos = []\n",
    "for item in raw_train:\n",
    "    tup = (item[0], item[1], 1)\n",
    "    trainPos.append(tup)\n",
    "    \n",
    "trainNeg = []\n",
    "for z in range(1):\n",
    "    for item in raw_train:\n",
    "        user = item[0]\n",
    "        games = gamesOfUser[user]\n",
    "        notPlayedGames = allGames - games\n",
    "        randomOne = random.sample(notPlayedGames, 1)[0]\n",
    "        tup = (user, randomOne, 0)\n",
    "        trainNeg.append(tup)\n",
    "\n",
    "len(trainPos), len(trainNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying validation set\n",
    "valid = []\n",
    "for item in raw_valid:\n",
    "    tup = (item[0], item[1], 1)\n",
    "    valid.append(tup)\n",
    "\n",
    "#Creating a negative validation elements and appending to validation set (valid)\n",
    "for item in raw_valid:\n",
    "    user = item[0]\n",
    "    games = gamesOfUser[user]\n",
    "    notPlayedGames = allGames - games\n",
    "    randomOne = random.sample(notPlayedGames, 1)[0]\n",
    "    tup = (user, randomOne, 0)\n",
    "    valid.append(tup)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Would-play baseline: just rank which games are popular and which are not, and return '1' if a game is among the top-ranked\n",
    "\n",
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user, game, _ in raw_train:\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort(reverse = True)\n",
    "\n",
    "maxi = 0\n",
    "for item in mostPopular:\n",
    "    maxi = max(maxi, item[0])\n",
    "\n",
    "mostPopularDict = defaultdict(float)\n",
    "for item in mostPopular:\n",
    "    mostPopularDict[item[1]] = item[0]/maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 67.905\n"
     ]
    }
   ],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/2: break\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for l in valid:\n",
    "    u, g, _ = l[0], l[1], l[2]\n",
    "    if g in return1:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])\n",
    "    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (Popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 69.925\n"
     ]
    }
   ],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    #Taking 68.9% of the top values (1/1.45)\n",
    "    if count > totalPlayed/1.45: break\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for l in valid:\n",
    "    u, g, _ = l[0], l[1], l[2]\n",
    "    if g in return1:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])\n",
    "    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (Jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 2 dicts. One with game and one with user\n",
    "\n",
    "gamesOfUser = defaultdict(set)\n",
    "usersOfGame = defaultdict(set)\n",
    "\n",
    "#Only on training set\n",
    "for item in raw_train:\n",
    "    gamesOfUser[item[0]].add(item[1])\n",
    "    usersOfGame[item[1]].add(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(u, g):\n",
    "    similarities1 = []\n",
    "    users = usersOfGame[g]\n",
    "    for g2 in gamesOfUser[u]:\n",
    "        if g == g2: continue # other than the query\n",
    "        sim = Jaccard(users, usersOfGame[g2])\n",
    "        similarities1.append(sim)\n",
    "    \n",
    "#     count = 0\n",
    "#     threshold = 0.008\n",
    "#     for sim in similarities1:\n",
    "#         if sim > threshold:\n",
    "#             count += 1\n",
    "    \n",
    "#     if count > 0.5 * len(similarities1):\n",
    "#         return 1\n",
    "\n",
    "    if len(similarities) == 0:\n",
    "        return \n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(u, g):\n",
    "    similarities1 = []\n",
    "    users = usersOfGame[g]\n",
    "    for g2 in gamesOfUser[u]:\n",
    "        if g == g2: continue # other than the query\n",
    "        sim = Jaccard(users, usersOfGame[g2])\n",
    "        similarities1.append(sim)\n",
    "    \n",
    "#     similarities2 = []\n",
    "#     games = gamesOfUser[u]\n",
    "#     for u2 in usersOfGame[g]:\n",
    "#         if u == u2: continue # other than the query\n",
    "#         sim = Jaccard(games, gamesOfUser[u2])\n",
    "#         similarities2.append(sim)\n",
    "    \n",
    "#     count = 0\n",
    "#     #Thershold set as 0.008\n",
    "#     threshold = 0.008\n",
    "#     for sim in similarities:\n",
    "#         if sim > threshold:\n",
    "#             count += 1\n",
    "    \n",
    "    #Since it is given most of the values should cross. Assuming it to be more than 50%.\n",
    "    #Can be done using median too\n",
    "#     if count > 0.5*len(similarities):\n",
    "#         return 1\n",
    "\n",
    "    if len(similarities1) > 0:\n",
    "        min1 = min(similarities1)\n",
    "    else:\n",
    "        min1 = 0.002459177358905974\n",
    "    \n",
    "    \n",
    "    \n",
    "    max1 = max(similarities1) if len(similarities1) > 0 else 0.036477232467427144\n",
    "    mean1 = sum(similarities1)/len(similarities1) if len(similarities1) > 0 else 0.013651729456640182\n",
    "    #median1 = median(similarities1) if len(similarities1) > 0 else 0\n",
    "    \n",
    "#     min2 = min(similarities2) if len(similarities2) > 0 else 0.008970185399297994\n",
    "#     max2 = max(similarities2) if len(similarities2) > 0 else 0.0960382978251585\n",
    "#     mean2 = sum(similarities2)/len(similarities2) if len(similarities2) > 0 else 0.03420200386502156\n",
    "    #median2 = median(similarities2) if len(similarities2) > 0 else 0\n",
    "    \n",
    "    pop = mostPopularDict[g]\n",
    "    if pop == 0:\n",
    "        pop = 0.12058540079119132\n",
    "    \n",
    "#     senti = [0, 0, 0, 0]\n",
    "#     if g in sentiDict:\n",
    "#         senti = sentiDict[g]\n",
    "    \n",
    "    return [1, pop, max1, min1, mean1] #, min2, max2, mean2+ senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "for tup in trainPos:\n",
    "    trainX.append(mostSimilar(tup[0], tup[1]))\n",
    "    trainY.append(tup[2])\n",
    "    \n",
    "for tup in trainNeg:\n",
    "    trainX.append(mostSimilar(tup[0], tup[1]))\n",
    "    trainY.append(tup[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min1All = 0\n",
    "# max1All = 0\n",
    "# mean1All = 0\n",
    "# min2All = 0\n",
    "# max2All = 0\n",
    "# mean2All = 0\n",
    "# popAll = 0\n",
    "# countpop = 0\n",
    "# count1 = 0\n",
    "# count2 = 0\n",
    "\n",
    "# for item in trainX:\n",
    "#     if item[1] != 0:\n",
    "#         countpop += 1\n",
    "#         popAll += item[1]\n",
    "#     if item[2] != 0:\n",
    "#         count1 += 1\n",
    "#         max1All += item[2]\n",
    "#         min1All += item[3]\n",
    "#         mean1All += item[4]\n",
    "#     if item[5] != 0:\n",
    "#         count2 += 1\n",
    "#         max2All += item[6]\n",
    "#         min2All += item[5]\n",
    "#         mean2All += item[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min1All /= count1\n",
    "# max1All /= count1\n",
    "# mean1All /= count1\n",
    "\n",
    "# min2All /= count2\n",
    "# max2All /= count2\n",
    "# mean2All /= count2\n",
    "\n",
    "# popAll /= countpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(min1All, max1All, mean1All, min2All, max2All, mean2All, popAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in trainX:\n",
    "#     if item[1] == 0:\n",
    "#         item[1] = popAll\n",
    "#     if item[2] == 0:\n",
    "#         item[2] = max1All\n",
    "#         item[3] = min1All\n",
    "#         item[4] = mean1All\n",
    "#     if item[5] == 0:\n",
    "#         item[6] = max2All\n",
    "#         item[5] = min2All\n",
    "#         item[7] = mean2All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrainX = np.matrix(trainX)\n",
    "ntrainy = np.array(trainY).T\n",
    "ntrainX.shape, ntrainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validX = []\n",
    "for item in valid:\n",
    "    validX.append(mostSimilar(item[0], item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = sklearn.linear_model.LogisticRegression(C = 0.05, class_weight='balanced')\n",
    "reg.fit(ntrainX, ntrainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validX = np.matrix(validX)\n",
    "predictions = reg.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Played.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Played_Out.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    testX = mostSimilar(u, g)\n",
    "    testX = np.matrix(testX)\n",
    "    res = reg.predict(testX)\n",
    "    f.write(u + '-' + g + ',' + str(res)[1:2] + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (Both Jaccard and Popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 71.93\n"
     ]
    }
   ],
   "source": [
    "out = 0\n",
    "from statistics import median\n",
    "predictions = []\n",
    "for item in valid:\n",
    "    #Only checking Jaccard if it is popular in the first place\n",
    "    if item[1] in return1:\n",
    "        predictions.append(mostSimilar(item[0], item[1]))\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "    \n",
    "actualValues = []\n",
    "for item in valid:\n",
    "    actualValues.append(item[2])    \n",
    "\n",
    "correctValues = [a == b for a, b in zip(predictions, actualValues)]\n",
    "\n",
    "print('Validation Accuracy: ' + str(sum(correctValues)/len(valid)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (Test Set and Submitting to Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Played.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Played_Final.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    if g in return1:\n",
    "        res = mostSimilar(u, g)\n",
    "    else:\n",
    "        res = 0\n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitted to Kaggle')\n",
    "print('\\nKaggle Username: aditya1c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import array\n",
    "import gzip\n",
    "import random\n",
    "from tensorflow.keras import Model\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON1(path):\n",
    "    for l in gzip.open(path, 'rt', encoding='UTF-8'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u, g, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation sets\n",
    "dataset = []\n",
    "for user, game, d in readJSON1(\"train.json.gz\"):\n",
    "    dataset.append([user, game, d['hours_transformed']])\n",
    "\n",
    "# random.shuffle(dataset)    \n",
    "    \n",
    "train = dataset[:175000]\n",
    "valid = dataset[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "intValid = []\n",
    "\n",
    "# Could adapt to any dataset, this one is from\n",
    "for ele in train:\n",
    "    u = ele[0]\n",
    "    i = ele[1]\n",
    "    r = ele[2]\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "    interactions.append((u,i,r))\n",
    "    \n",
    "for ele in valid:\n",
    "    u = ele[0]\n",
    "    i = ele[1]\n",
    "    r = ele[2]\n",
    "    intValid.append((u,i,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rating, just for initialization\n",
    "mu = sum([r for _,_,r in interactions]) / len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent optimizer, experiment with learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentFactorModel(tf.keras.Model):\n",
    "    def __init__(self, mu, K, lamb1, lamb2):\n",
    "        super(LatentFactorModel, self).__init__()\n",
    "        # Initialize to average\n",
    "        self.alpha = tf.Variable(mu)\n",
    "        # Initialize to small random values\n",
    "        self.betaU = tf.Variable(tf.random.normal([len(userIDs)],stddev=0.1))\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.1))\n",
    "        \n",
    "        self.betaU.assign(list(userBiases.values()))\n",
    "        self.betaI.assign(list(gameBiases.values()))\n",
    "        \n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.1))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.1))\n",
    "        self.lamb1 = lamb1\n",
    "        self.lamb2 = lamb2\n",
    "\n",
    "    # Prediction for a single instance (useful for evaluation)\n",
    "    def predict(self, u, i):\n",
    "        p = self.alpha + self.betaU[u] + self.betaI[i] +\\\n",
    "            tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb1 * (tf.reduce_sum(self.betaU**2) + tf.reduce_sum(self.betaI**2)) + self.lamb2 * (tf.reduce_sum(self.gammaU**2) + tf.reduce_sum(self.gammaI**2))\n",
    "    \n",
    "    # Prediction for a sample of instances\n",
    "    def predictSample(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_u = tf.nn.embedding_lookup(self.betaU, u)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        pred = self.alpha + beta_u + beta_i +\\\n",
    "               tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return pred\n",
    "    \n",
    "    # Loss\n",
    "    def call(self, sampleU, sampleI, sampleR):\n",
    "        pred = self.predictSample(sampleU, sampleI)\n",
    "        r = tf.convert_to_tensor(sampleR, dtype=tf.float32)\n",
    "        return tf.nn.l2_loss(pred - r) / len(sampleR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with number of factors and regularization rate\n",
    "model = LatentFactorModel(3.1164, 8, 0.00001, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(interactions):\n",
    "    Nsamples = 175000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleR = [], [], []\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,r = random.choice(interactions)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleR.append(r)\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleR)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, objective = 5.109024\n",
      "iteration 1, objective = 2.8393712\n",
      "iteration 2, objective = 2.6481733\n",
      "iteration 3, objective = 3.6970587\n",
      "iteration 4, objective = 3.3134131\n",
      "iteration 5, objective = 2.5834265\n",
      "iteration 6, objective = 2.5802207\n",
      "iteration 7, objective = 2.540879\n",
      "iteration 8, objective = 2.2537599\n",
      "iteration 9, objective = 2.2860477\n",
      "iteration 10, objective = 2.3310444\n",
      "iteration 11, objective = 2.1392677\n",
      "iteration 12, objective = 1.9966738\n",
      "iteration 13, objective = 2.0006428\n",
      "iteration 14, objective = 1.9733336\n",
      "iteration 15, objective = 1.8927789\n",
      "iteration 16, objective = 1.837797\n",
      "iteration 17, objective = 1.8162799\n",
      "iteration 18, objective = 1.7826585\n",
      "iteration 19, objective = 1.7538829\n",
      "iteration 20, objective = 1.712073\n",
      "iteration 21, objective = 1.6775392\n",
      "iteration 22, objective = 1.681324\n",
      "iteration 23, objective = 1.645169\n",
      "iteration 24, objective = 1.6233385\n",
      "iteration 25, objective = 1.604901\n",
      "iteration 26, objective = 1.6020132\n",
      "iteration 27, objective = 1.5819721\n",
      "iteration 28, objective = 1.5706064\n",
      "iteration 29, objective = 1.5505924\n",
      "iteration 30, objective = 1.5485343\n",
      "iteration 31, objective = 1.5384126\n",
      "iteration 32, objective = 1.5304581\n",
      "iteration 33, objective = 1.5181025\n",
      "iteration 34, objective = 1.5203168\n",
      "iteration 35, objective = 1.5164142\n",
      "iteration 36, objective = 1.5061377\n",
      "iteration 37, objective = 1.5064814\n",
      "iteration 38, objective = 1.4960921\n",
      "iteration 39, objective = 1.5048769\n",
      "iteration 40, objective = 1.4911352\n",
      "iteration 41, objective = 1.4793147\n",
      "iteration 42, objective = 1.4857389\n",
      "iteration 43, objective = 1.4838392\n",
      "iteration 44, objective = 1.4678195\n",
      "iteration 45, objective = 1.473763\n",
      "iteration 46, objective = 1.476166\n",
      "iteration 47, objective = 1.4671378\n",
      "iteration 48, objective = 1.4697704\n",
      "iteration 49, objective = 1.4630253\n",
      "iteration 50, objective = 1.4718817\n",
      "iteration 51, objective = 1.4715279\n",
      "iteration 52, objective = 1.4566001\n",
      "iteration 53, objective = 1.465856\n",
      "iteration 54, objective = 1.4609382\n",
      "iteration 55, objective = 1.456591\n",
      "iteration 56, objective = 1.4574856\n",
      "iteration 57, objective = 1.4556123\n",
      "iteration 58, objective = 1.4619435\n",
      "iteration 59, objective = 1.4528035\n",
      "iteration 60, objective = 1.4480269\n",
      "iteration 61, objective = 1.4561334\n",
      "iteration 62, objective = 1.4701681\n",
      "iteration 63, objective = 1.4592366\n",
      "iteration 64, objective = 1.4471014\n",
      "iteration 65, objective = 1.4644626\n",
      "iteration 66, objective = 1.4629525\n",
      "iteration 67, objective = 1.4567587\n",
      "iteration 68, objective = 1.4585127\n",
      "iteration 69, objective = 1.4573293\n",
      "iteration 70, objective = 1.4682335\n",
      "iteration 71, objective = 1.4459075\n",
      "iteration 72, objective = 1.4542549\n",
      "iteration 73, objective = 1.453735\n",
      "iteration 74, objective = 1.452122\n",
      "iteration 75, objective = 1.4422475\n",
      "iteration 76, objective = 1.449857\n",
      "iteration 77, objective = 1.4531666\n",
      "iteration 78, objective = 1.4638137\n",
      "iteration 79, objective = 1.4465476\n"
     ]
    }
   ],
   "source": [
    "# 100 iterations of gradient descent\n",
    "for i in range(80):\n",
    "    obj = trainingStep(interactions)\n",
    "    print(\"iteration \" + str(i) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8073114928465777"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "for u, g, h in intValid:\n",
    "    pred.append(model.predict(userIDs[u], itemIDs[g]).numpy())\n",
    "    actual.append(h)\n",
    "    \n",
    "mse = []\n",
    "for i in range(len(pred)):\n",
    "    mse.append((pred[i] - actual[i])**2)\n",
    "    \n",
    "mse = sum(mse)/len(mse)\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Hours.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Hours_Out.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    res = model.predict(userIDs[u], itemIDs[g]).numpy()\n",
    "    #This can be replaced with 0 if required too. Just used for checking in Kaggle\n",
    "#     if u not in userBiases or g not in gameBiases:\n",
    "#         res = ratingMean\n",
    "        \n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursPerUser = defaultdict(list) \n",
    "hoursPerGame = defaultdict(list)\n",
    "allHours = []\n",
    "\n",
    "for user, game, h in train:\n",
    "    allHours.append(h)\n",
    "    #These variables are not used much. Just to maintain consistency with workbook code\n",
    "    hoursPerUser[user].append(h)\n",
    "    hoursPerGame[game].append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataset)\n",
    "nUsers = len(hoursPerUser)\n",
    "nGames = len(hoursPerGame)\n",
    "users = list(hoursPerUser.keys())\n",
    "games = list(hoursPerGame.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sum(allHours)/len(allHours)\n",
    "userBiases = defaultdict(float)\n",
    "gameBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Functions which are similar to those discussed in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, game):\n",
    "    return alpha + userBiases[user] + gameBiases[game]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global gameBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    gameBiases = dict(zip(games, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(u, g) for u, g, h in dataset]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in gameBiases:\n",
    "        cost += lamb*gameBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(dataset)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dGameBiases = defaultdict(float)\n",
    "    for u, g, h in dataset:\n",
    "        u,i = u, g\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - h\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dGameBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in gameBiases:\n",
    "        dGameBiases[i] += 2*lamb*gameBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dGameBiases[i] for i in games]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on training set with lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 5.280231958987971\n",
      "MSE = 5.161276766723984\n",
      "MSE = 4.75220537977447\n",
      "MSE = 15.497401226086064\n",
      "MSE = 4.559106199125477\n",
      "MSE = 4.07024982602844\n",
      "MSE = 4.057352528717888\n",
      "MSE = 4.00751662481554\n",
      "MSE = 3.8362255746403258\n",
      "MSE = 3.495404784862\n",
      "MSE = 3.3151372824830214\n",
      "MSE = 3.110933249679368\n",
      "MSE = 3.026955091253423\n",
      "MSE = 2.931702174551582\n",
      "MSE = 2.875921278902824\n",
      "MSE = 2.838281637606661\n",
      "MSE = 2.8270300833697988\n",
      "MSE = 2.8165051834919237\n",
      "MSE = 4.138396012442667\n",
      "MSE = 2.816345248025124\n",
      "MSE = 2.809047274049851\n",
      "MSE = 2.7975875941133492\n",
      "MSE = 2.796215105085698\n",
      "MSE = 2.7938008029820147\n",
      "MSE = 2.7909121016047473\n",
      "MSE = 2.788390937912183\n",
      "MSE = 2.7857278840085944\n",
      "MSE = 2.7864035436939614\n",
      "MSE = 2.785643915804261\n",
      "MSE = 2.7854691499232236\n",
      "MSE = 2.7852270314773064\n",
      "MSE = 2.786018988600938\n",
      "MSE = 2.785224958222594\n",
      "MSE = 2.784961537714502\n",
      "MSE = 2.78491255777868\n",
      "MSE = 2.7848542453288756\n",
      "MSE = 2.7848438273236487\n",
      "MSE = 2.7844494040317658\n",
      "MSE = 2.784492887688483\n",
      "MSE = 2.784405497814387\n",
      "MSE = 2.784205426490971\n",
      "MSE = 2.784070250144463\n",
      "MSE = 2.7838562199232393\n",
      "MSE = 2.783749737211106\n",
      "MSE = 2.783816333271853\n",
      "MSE = 2.7838065366355034\n",
      "MSE = 2.7838096956473395\n",
      "MSE = 2.7838299792231167\n",
      "MSE = 2.783846007077037\n",
      "MSE = 2.7838725331316763\n",
      "MSE = 2.7838523619142\n",
      "MSE = 2.7838584022553103\n",
      "MSE = 2.7838422885889016\n",
      "MSE = 2.783818601741427\n",
      "MSE = 2.7837934490009393\n",
      "MSE = 2.783770831791199\n",
      "MSE = 2.783740217591912\n",
      "MSE = 2.7836868110296376\n",
      "MSE = 2.7837376414248083\n",
      "MSE = 2.7837393760063285\n",
      "MSE = 2.7837480435624204\n",
      "MSE = 2.78374201549524\n",
      "MSE = 2.7837485855321153\n",
      "MSE = 2.7837531487303653\n",
      "MSE = 2.7837579549351252\n",
      "MSE = 2.783769430797514\n",
      "MSE = 2.783749393962326\n",
      "MSE = 2.7837628694013583\n",
      "MSE = 2.783773969414404\n",
      "MSE = 2.7837781994877977\n",
      "MSE = 2.783775824219372\n",
      "MSE = 2.78377310688087\n",
      "MSE = 2.78377639803746\n",
      "MSE = 2.7837783053303684\n",
      "MSE = 2.783781221058449\n",
      "MSE = 2.7837863502880027\n",
      "MSE = 2.7837892770087245\n",
      "MSE = 2.783801024195914\n",
      "MSE = 2.78381612308114\n",
      "MSE = 2.7838346068850943\n",
      "MSE = 2.7838194699698993\n",
      "MSE = 2.7838218035399294\n",
      "MSE = 2.7838007112114047\n",
      "MSE = 2.783814581500401\n",
      "MSE = 2.7838134689210214\n",
      "MSE = 2.7838120318237456\n",
      "MSE = 2.7838109641258306\n",
      "MSE = 2.7838113074867326\n",
      "MSE = 2.7838157083012627\n",
      "MSE = 2.7838468566301144\n",
      "MSE = 2.783817699250022\n",
      "MSE = 2.7838221324942727\n",
      "MSE = 2.783825912855166\n",
      "MSE = 2.783825935143605\n",
      "MSE = 2.783824973842731\n",
      "MSE = 2.7838181842014613\n",
      "MSE = 2.783822612191787\n",
      "MSE = 2.783821995058298\n",
      "MSE = 2.7838219041547925\n",
      "MSE = 2.783821928197174\n",
      "MSE = 2.783793764533391\n",
      "MSE = 2.783814523270139\n",
      "MSE = 2.78381286928023\n",
      "MSE = 2.783806305716476\n",
      "MSE = 2.7837999654426215\n",
      "MSE = 2.7838041296012426\n",
      "MSE = 2.7838059686778918\n",
      "MSE = 2.7838356191547087\n",
      "MSE = 2.7838120765946073\n",
      "MSE = 2.7838140773225364\n",
      "MSE = 2.783813720840883\n",
      "MSE = 2.783806273777243\n",
      "MSE = 2.783807227874163\n",
      "MSE = 2.7838065848203826\n",
      "MSE = 2.7838054615292256\n",
      "MSE = 2.783805482151846\n",
      "MSE = 2.7837990943133186\n",
      "MSE = 2.7838030138736953\n",
      "MSE = 2.783802220679279\n",
      "MSE = 2.7838032150236813\n",
      "MSE = 2.783803642309327\n",
      "MSE = 2.7838068751187417\n",
      "MSE = 2.783742458989831\n",
      "MSE = 2.7838045371202527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.11527399, -1.1599159 ,  0.41032202, ..., -0.27174604,\n",
       "        -0.73349262, -1.62234258]),\n",
       " 2.9206101252145085,\n",
       " {'grad': array([-6.07451834e-06, -3.88092616e-07, -5.96837378e-08, ...,\n",
       "         -4.94227630e-08,  2.38215942e-08, -4.98864542e-07]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 124,\n",
       "  'nit': 100,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nGames),\n",
    "                             derivative, args = (allHours, 0.000029))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsValid = []\n",
    "for u, g, h in valid:\n",
    "    predictionsValid.append(prediction(u, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Validation Set with lambda as 1 is 2.749940733410276\n"
     ]
    }
   ],
   "source": [
    "mseValid = MSE(predictionsValid, allHoursValid)\n",
    "print('MSE on Validation Set with lambda as 1 is ' + str(mseValid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Hours.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Hours_Out_2.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    res = prediction(u, g)\n",
    "    #This can be replaced with 0 if required too. Just used for checking in Kaggle\n",
    "    if u not in userBiases or g not in gameBiases:\n",
    "        res = ratingMean\n",
    "        \n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting using lambda and key\n",
    "sorted_users = sorted(userBiases.items(), key = lambda x: x[1])\n",
    "smallestUser = sorted_users[0]\n",
    "largestUser = sorted_users[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_games = sorted(gameBiases.items(), key = lambda x: x[1])\n",
    "smallestGame = sorted_games[0]\n",
    "largestGame = sorted_games[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest User Bias: ('u04589037', -3.03607644319556)\n",
      "\n",
      "Largest User Bias: ('u38845867', 6.2844331857368285)\n"
     ]
    }
   ],
   "source": [
    "print('Smallest User Bias: ' + str(smallestUser))\n",
    "print('\\nLargest User Bias: ' + str(largestUser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Game Bias: ('b05546112', -2.8090516714749536)\n",
      "\n",
      "Largest Game Bias: ('b80191205', 5.534327809661798)\n"
     ]
    }
   ],
   "source": [
    "print('Smallest Game Bias: ' + str(smallestGame))\n",
    "print('\\nLargest Game Bias: ' + str(largestGame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for lambda from 0 to 1 in steps of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 5.696797120965593\n",
      "MSE = 5.3868176525322236\n",
      "MSE = 5.262014825224892\n",
      "MSE = 5.254219187298708\n",
      "MSE = 5.225676100566427\n",
      "MSE = 4.946000296618459\n",
      "MSE = 4.575342529889029\n",
      "MSE = 4.204257517553415\n",
      "MSE = 4.0251737458044134\n",
      "MSE = 3.9696241872528866\n",
      "MSE = 3.9515104511061407\n",
      "MSE = 3.8985446906103958\n",
      "MSE = 3.807113313669612\n",
      "MSE = 3.6636070522601547\n",
      "MSE = 3.5417041934016296\n",
      "MSE = 3.339520688184969\n",
      "MSE = 3.2893828833599033\n",
      "MSE = 3.2006075872201283\n",
      "MSE = 3.1447864417237033\n",
      "MSE = 3.113303618570252\n",
      "MSE = 3.0936696227260625\n",
      "MSE = 3.7636476670203343\n",
      "MSE = 3.085245997819632\n",
      "MSE = 3.0561949440034257\n",
      "MSE = 2.984427315945522\n",
      "MSE = 2.946546266980766\n",
      "MSE = 2.9009488975906375\n",
      "MSE = 3.0971629162698884\n",
      "MSE = 2.8936863021866426\n",
      "MSE = 2.8769489790075893\n",
      "MSE = 2.854174723086478\n",
      "MSE = 2.8452075420192053\n",
      "MSE = 2.8407238983980836\n",
      "MSE = 2.838215237375079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fe21a7dd883a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtmpLamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nGames),\n\u001b[0;32m---> 12\u001b[0;31m                              derivative, args = (allHours, tmpLamb), disp = False)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mpredictionsValid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 198\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    199\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    200\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFD_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.6/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mgrad_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mgrad_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-680c56c5480f>\u001b[0m in \u001b[0;36mderivative\u001b[0;34m(theta, labels, lamb)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdalpha\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdUserBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdGameBiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserBiases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bestLambda = 0\n",
    "bestMSE = float('inf')\n",
    "lambdaList = []\n",
    "mseList = []\n",
    "\n",
    "lamb = 0\n",
    "step = 0.05\n",
    "#Training and checking using Lambda from 0 to 1\n",
    "for i in range(int(1/step) + 1):\n",
    "    tmpLamb = lamb + 0.05*i\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nGames),\n",
    "                             derivative, args = (allHours, tmpLamb), disp = False)\n",
    "    predictionsValid = []\n",
    "    for u, g, h in valid:\n",
    "        predictionsValid.append(prediction(u, g))\n",
    "    mseValid = MSE(predictionsValid, allHoursValid)\n",
    "    lambdaList.append(tmpLamb)\n",
    "    mseList.append(mseValid)\n",
    "    if mseValid < bestMSE:\n",
    "        bestMSE = mseValid\n",
    "        bestLambda = tmpLamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code block output is run locally but not showed it here as the output is too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Lambda: ' + str(bestLambda))\n",
    "print('Best MSE: ' + str(bestMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lambdaList, mseList)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Variation of MSE with Lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model is used to predict the test set and uploaded to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('pairs_Hours.txt', 'r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('pairs_Hours_Out_2.txt', 'w')\n",
    "f.write('userID-gameID,prediction\\n')\n",
    "lines = lines[1:]\n",
    "for line in lines:\n",
    "    u, g = line.strip().split('-')\n",
    "    res = prediction(u, g)\n",
    "    #This can be replaced with 0 if required too. Just used for checking in Kaggle\n",
    "    if u not in userBiases or g not in gameBiases:\n",
    "        res = ratingMean\n",
    "        \n",
    "    f.write(u + '-' + g + ',' + str(res) + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitted to Kaggle')\n",
    "print('\\nKaggle Username: aditya1c')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
